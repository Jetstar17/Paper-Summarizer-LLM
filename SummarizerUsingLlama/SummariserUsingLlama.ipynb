{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8230136,"sourceType":"datasetVersion","datasetId":4880117},{"sourceId":8724768,"sourceType":"datasetVersion","datasetId":5235908},{"sourceId":8743213,"sourceType":"datasetVersion","datasetId":5249561}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langchain_huggingface\n!pip install langchain_community\n!pip install gputil\n!pip install PyMuPDF\n!pip install pymilvus\n!pip install sentence-transformers\n!pip install ctransformers\n!pip install faiss-cpu\n!pip install numpy==1.26.4\n!pip install langchain_milvus\n!pip install pymilvus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import LlamaCpp\nfrom langchain_community.chat_models import ChatLlamaCpp\nfrom langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\nimport os\nimport json\nimport warnings\nimport logging\nimport time\nimport psutil\nimport GPUtil\n\nwarnings.filterwarnings(\"ignore\")\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)  # Adjust level as needed\nlogger = logging.getLogger(__name__)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install arxiv  llama-cpp-python","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from arxiv import Search\n\n\n\n# Initialize Llama CPP\nllm = LlamaCpp(\n            model_path=\"/kaggle/input/meta-llama-3-8b-gguf/llama3-8b-instruct-Q5_K_M.gguf\",\n            n_ctx=2048,\n            n_gpu_layers=-1,\n            verbose=True,\n            seed=1000,\n            temperature=0,\n        )\n\n\ndef fetch_arxiv_data(query):\n    # Fetching papers from arxiv.org\n    search = Search(\n        query=query,\n        max_results=5, \n        \n    )\n    papers = list(search.results())\n    sorted_papers = sorted(papers, key=lambda paper: paper.published, reverse=True)  # Sort by publication date\n\n    return sorted_papers\n\n    \n\ndef generate_summary(query):\n    # Fetch papers\n    papers = fetch_arxiv_data(query)\n\n    # Process each paper\n    summaries = []\n    for paper in papers:\n        title = paper.title\n        abstract = paper.summary\n\n\n        # Generate detailed content using Llama CPP\n        prompt_template = PromptTemplate(template=\"Provide a detailed overview based on the title: {title}\", input_variables=['title'])\n        prompt_det = prompt_template.format(title=title)\n        detailed_content = llm.invoke(prompt_det)\n\n\n        summary_prompt = PromptTemplate(template=\"Summarize the following text: {title}. {abstract}. {detailed_content}\", input_variables=['title', 'abstract', 'detailed_content'])\n        prompt_summary =  summary_prompt.format(title = title, abstract = abstract, detailed_content = detailed_content)\n        prompt_sum = prompt_summary\n\n        # Generate summary using Llama CPP\n        summary = llm.invoke(prompt_sum)\n\n        summaries.append({\n            'title': title,\n            'original_abstract': abstract,\n            'generated_summary': summary\n        })\n\n    return summaries\n\n# Example usage:\nquery = \"Naruto Uzumaki\"\nsummaries = generate_summary(query)\n\nfor summary in summaries:\n    print(f\"Title: {summary['title']}\")\n    print(f\"Original Abstract: {summary['original_abstract']}\")\n    print(f\"Generated Summary: {summary['generated_summary']}\")\n    print(\"\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}