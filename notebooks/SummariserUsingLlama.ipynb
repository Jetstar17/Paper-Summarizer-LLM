{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8847029,"sourceType":"datasetVersion","datasetId":5324960}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rishitjakharia/research-paper-summarizer?scriptVersionId=187214742\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# RAG Based LLM Application Made using Llama-3-8b-gguf\n---\nThis Application should help provide summary of a reseearch paper when given the name of the research paper","metadata":{}},{"cell_type":"markdown","source":"## Tech Stack Used\n----\n1) arXiv\n\n2) LlamaCpp\n\n3) sentence-transformers/all-MiniLM-L6-v2","metadata":{}},{"cell_type":"markdown","source":"## Pages\n---\n1) Downloading Relevant libraries/models\n\n2) Importing Files\n\n3) File Path\n\n4) RAG","metadata":{}},{"cell_type":"markdown","source":"## Downloading relevant libraries/models","metadata":{}},{"cell_type":"code","source":"!pip install langchain_huggingface\n!pip install langchain_community\n!pip install arxiv\n!pip install sentence-transformers\n!pip install faiss-cpu\n!pip install numpy==1.26.4","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:43:48.111592Z","iopub.execute_input":"2024-07-07T12:43:48.112188Z","iopub.status.idle":"2024-07-07T12:45:15.188845Z","shell.execute_reply.started":"2024-07-07T12:43:48.112156Z","shell.execute_reply":"2024-07-07T12:45:15.187616Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting langchain_huggingface\n  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.23.2)\nCollecting langchain-core<0.3,>=0.1.52 (from langchain_huggingface)\n  Downloading langchain_core-0.2.11-py3-none-any.whl.metadata (6.0 kB)\nCollecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.19.1)\nRequirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\nCollecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.1.52->langchain_huggingface)\n  Downloading langsmith-0.1.83-py3-none-any.whl.metadata (13 kB)\nCollecting packaging>=20.9 (from huggingface-hub>=0.23.0->langchain_huggingface)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.5.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.2.3)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.11.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (9.5.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.4)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\nDownloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\nDownloading langchain_core-0.2.11-py3-none-any.whl (337 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.83-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, sentence-transformers, langchain_huggingface\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-core-0.2.11 langchain_huggingface-0.0.3 langsmith-0.1.83 orjson-3.10.6 packaging-24.1 sentence-transformers-3.0.1\nCollecting langchain_community\n  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.6)\nCollecting langchain<0.3.0,>=0.2.6 (from langchain_community)\n  Downloading langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.2.11)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.1.83)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.6->langchain_community)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (2.5.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (24.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.9.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_community) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nDownloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.2.6-py3-none-any.whl (975 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nInstalling collected packages: langchain-text-splitters, langchain, langchain_community\nSuccessfully installed langchain-0.2.6 langchain-text-splitters-0.2.2 langchain_community-0.2.6\nCollecting arxiv\n  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\nCollecting feedparser~=6.0.10 (from arxiv)\n  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: requests~=2.32.0 in /opt/conda/lib/python3.10/site-packages (from arxiv) (2.32.3)\nCollecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2024.2.2)\nDownloading arxiv-2.1.3-py3-none-any.whl (11 kB)\nDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=e75d5f46beace3caafdef9678603a53cb556de3128431443943b2181d9008518\n  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\nSuccessfully built sgmllib3k\nInstalling collected packages: sgmllib3k, feedparser, arxiv\nSuccessfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: numpy<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (24.1)\nDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.8.0.post1\nRequirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.10/site-packages (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings\nEMB = HuggingFaceEmbeddings(\n    model_name='sentence-transformers/all-MiniLM-L6-v2', \n    model_kwargs=\n    {\n        'device': 'cuda'\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:45:15.191149Z","iopub.execute_input":"2024-07-07T12:45:15.191489Z","iopub.status.idle":"2024-07-07T12:45:40.627188Z","shell.execute_reply.started":"2024-07-07T12:45:15.191459Z","shell.execute_reply":"2024-07-07T12:45:40.626345Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n2024-07-07 12:45:24.474054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-07 12:45:24.474190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-07 12:45:24.607153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c8d852ed03041cdbbe1b422e12402fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b64b0434652145bca6c48e947132c187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b0390895eb4edf8e983e318fabddae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd82d0a69a2440d6ad59c1368cb25b3c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c25d2a14884520b0cce4a7dded7b22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8f2d46b1a2641eb8b8a1c16eba3673b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9024fe89bfe94fe2a0f5b2aead18be30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"955a2c0c6d234028acea2c665f533801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0103435ae2c545119df78b956db8f05a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398325a4639e4d17a433767786b9dff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db4a4fe50b04ec4b6c4b11ebd74d698"}},"metadata":{}}]},{"cell_type":"code","source":"!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.77 -U --force-reinstall --no-cache-dir --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:45:40.628343Z","iopub.execute_input":"2024-07-07T12:45:40.629075Z","iopub.status.idle":"2024-07-07T12:46:21.784172Z","shell.execute_reply.started":"2024-07-07T12:45:40.629025Z","shell.execute_reply":"2024-07-07T12:46:21.783071Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\nCollecting llama-cpp-python==0.2.77\n  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.77-cu121/llama_cpp_python-0.2.77-cp310-cp310-linux_x86_64.whl (326.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.6/326.6 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.77)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting numpy>=1.20.0 (from llama-cpp-python==0.2.77)\n  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.77)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nCollecting jinja2>=2.11.3 (from llama-cpp-python==0.2.77)\n  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\nCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.2.77)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m230.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nInstalling collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.1.3\n    Uninstalling MarkupSafe-2.1.3:\n      Successfully uninstalled MarkupSafe-2.1.3\n  Attempting uninstall: jinja2\n    Found existing installation: Jinja2 3.1.2\n    Uninstalling Jinja2-3.1.2:\n      Successfully uninstalled Jinja2-3.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 2.0.0 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\narviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 2.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\ncontourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.0 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ncudf 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\ncupy 13.1.0 requires numpy<1.29,>=1.22, but you have numpy 2.0.0 which is incompatible.\ndask-cuda 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\ndask-cudf 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nfaiss-cpu 1.8.0.post1 requires numpy<2.0,>=1.0, but you have numpy 2.0.0 which is incompatible.\nfitter 1.7.0 requires numpy<2.0.0,>=1.20.0, but you have numpy 2.0.0 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlangchain 0.2.6 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\nlangchain-community 0.2.6 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmatplotlib 3.7.5 requires numpy<2,>=1.20, but you have numpy 2.0.0 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nnumba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npandas 2.2.1 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.0.0 which is incompatible.\npylibraft 24.4.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\npylibraft 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\npywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.0.0 which is incompatible.\nraft-dask 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nrmm 24.4.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nrmm 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\nscipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nstatsmodels 0.14.1 requires numpy<2,>=1.18, but you have numpy 2.0.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\ntensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\ntensorflow-transform 0.14.0 requires numpy<2,>=1.16, but you have numpy 2.0.0 which is incompatible.\ntensorstore 0.1.60 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nucx-py 0.37.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.77 numpy-1.26.4 typing-extensions-4.12.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing Files","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import LlamaCpp\nfrom langchain_community.chat_models import ChatLlamaCpp\nfrom langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\nfrom arxiv import Search\nimport pandas as pd\nimport numpy as np\nimport os\nimport json\nimport warnings\nimport logging\nimport time\nimport psutil\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:47:03.564746Z","iopub.execute_input":"2024-07-07T12:47:03.565138Z","iopub.status.idle":"2024-07-07T12:47:03.571228Z","shell.execute_reply.started":"2024-07-07T12:47:03.565109Z","shell.execute_reply":"2024-07-07T12:47:03.570098Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## File Path","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:47:05.774887Z","iopub.execute_input":"2024-07-07T12:47:05.775728Z","iopub.status.idle":"2024-07-07T12:47:05.789119Z","shell.execute_reply.started":"2024-07-07T12:47:05.775697Z","shell.execute_reply":"2024-07-07T12:47:05.788092Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/meta-llama/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RAG","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO) \nlogger = logging.getLogger(__name__)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:47:06.661563Z","iopub.execute_input":"2024-07-07T12:47:06.662527Z","iopub.status.idle":"2024-07-07T12:47:06.666955Z","shell.execute_reply.started":"2024-07-07T12:47:06.662495Z","shell.execute_reply":"2024-07-07T12:47:06.665992Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class QAbot():\n    def __init__(self, prompt, Embeddings):\n        self.EMBEDDINGS = Embeddings\n        self.custom_prompt_template = prompt\n        self.callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n        self.llm = self.load_llm()\n        self.refer = \"\"\n        \n    # setting prompt wrapper\n    def set_custom_prompt(self, context: str, question: str):\n        \"\"\"\n        Create a prompt using the provided context and question.\n        \"\"\"\n        prompt_template = PromptTemplate(template=self.custom_prompt_template, input_variables=['context', 'question'])\n        prompt = prompt_template.format(context=context, question=question)\n        logger.info(\"The prompt was created.\")\n        print(\"Prompt Created\")\n        return prompt\n    \n    # Fetching papers from arxiv.org\n    def fetch_arxiv_data(self, query):\n        search = Search(\n            query=query,\n            max_results=1, \n        )\n        papers = list(search.results())\n        sorted_papers = sorted(papers, key=lambda paper: paper.published, reverse=True)  # Sort by publication date\n        logger.info(\"The data retrieved was created.\")\n        print(\"Papers Retrieved Created\")\n        return sorted_papers\n    \n    # Loading the model\n    def load_llm(self):\n        \"\"\"\n        Loading the LLM using Llama Cpp.\n        \"\"\"\n        llm = LlamaCpp(\n            model_path=\"/kaggle/input/meta-llama/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf\",\n            callback_manager = self.callback_manager,\n            n_ctx=3072,\n            max_tokens = 4096,\n            n_gpu_layers=-1,\n            verbose=True,\n            flash_attn=True,\n            chat_format = \"llama-3\"\n        )\n        logger.info(\"LLM model loaded.\")\n        print(\"LLM Model Loaded\")\n        return llm\n\n    # loading the qa_bot wrapper\n    def qa_bot(self, question: str):\n        \"\"\"\n        Manually perform the search, create the prompt, and get the response from the LLM.\n        \"\"\"\n        arxiv_data = self.fetch_arxiv_data(question)\n        context = \"\"\n        for papers in arxiv_data:\n            context += \"**\" + papers.title + \"**\\n\"\n            context += papers.summary + \"\\n\\n\"\n        \n        prompt = self.set_custom_prompt(context, question)\n        print(f\"Prompt\\n{prompt}\\n-----------------------------------\")\n        return self.llm.invoke(prompt)\n    \n    def get_reference(self):\n        return self.refer","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:47:07.541603Z","iopub.execute_input":"2024-07-07T12:47:07.541928Z","iopub.status.idle":"2024-07-07T12:47:07.554666Z","shell.execute_reply.started":"2024-07-07T12:47:07.541903Z","shell.execute_reply":"2024-07-07T12:47:07.553681Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"\n    based on the following metadata from arxiv.org\n    {context}\n    provide a text summary for the research papers.\n    in format\n    ## Title\n    - content in points\n\"\"\"\n\nqa_instance = QAbot(prompt, EMB)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:47:09.917103Z","iopub.execute_input":"2024-07-07T12:47:09.917471Z","iopub.status.idle":"2024-07-07T12:48:13.155623Z","shell.execute_reply.started":"2024-07-07T12:47:09.917441Z","shell.execute_reply":"2024-07-07T12:48:13.154688Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /kaggle/input/meta-llama/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 17\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\nllama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\nllama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\nllama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q5_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.8000 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q5_K - Medium\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 5.33 GiB (5.70 BPW) \nllm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\nggml_cuda_init: CUDA_USE_TENSOR_CORES: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.30 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   344.44 MiB\nllm_load_tensors:      CUDA0 buffer size =  5115.49 MiB\n.........................................................................................\nllama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\nllama_new_context_with_model: n_ctx      = 3072\nllama_new_context_with_model: n_batch    = 32\nllama_new_context_with_model: n_ubatch   = 32\nllama_new_context_with_model: flash_attn = 1\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   384.00 MiB\nllama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =    16.16 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =     0.88 MiB\nllama_new_context_with_model: graph nodes  = 903\nllama_new_context_with_model: graph splits = 2\n","output_type":"stream"},{"name":"stdout","text":"LLM Model Loaded\n","output_type":"stream"},{"name":"stderr","text":"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '17', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\nAvailable chat formats from metadata: chat_template.default\n","output_type":"stream"}]},{"cell_type":"code","source":"def main():\n    start_time = time.time()\n    ans = []\n    question = input(\"Enter your question below\\n\")\n    final = \"\"\n    l = []\n    for chunk in qa_instance.qa_bot(question):\n        l.append(time.time())\n        final += str(chunk)\n    _o = qa_instance.get_reference()\n    ans.append([question, final, _o])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:48:24.086881Z","iopub.execute_input":"2024-07-07T12:48:24.088044Z","iopub.status.idle":"2024-07-07T12:48:24.093685Z","shell.execute_reply.started":"2024-07-07T12:48:24.087994Z","shell.execute_reply":"2024-07-07T12:48:24.092655Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T12:48:24.989857Z","iopub.execute_input":"2024-07-07T12:48:24.990881Z","iopub.status.idle":"2024-07-07T12:48:53.853607Z","shell.execute_reply.started":"2024-07-07T12:48:24.99085Z","shell.execute_reply":"2024-07-07T12:48:53.852728Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your question below\n NARUTO\n"},{"name":"stdout","text":"Papers Retrieved Created\nPrompt Created\nPrompt\n\n    based on the following metadata from arxiv.org\n    **NARUTO: Neural Active Reconstruction from Uncertain Target Observations**\nWe present NARUTO, a neural active reconstruction system that combines a\nhybrid neural representation with uncertainty learning, enabling high-fidelity\nsurface reconstruction. Our approach leverages a multi-resolution hash-grid as\nthe mapping backbone, chosen for its exceptional convergence speed and capacity\nto capture high-frequency local features.The centerpiece of our work is the\nincorporation of an uncertainty learning module that dynamically quantifies\nreconstruction uncertainty while actively reconstructing the environment. By\nharnessing learned uncertainty, we propose a novel uncertainty aggregation\nstrategy for goal searching and efficient path planning. Our system\nautonomously explores by targeting uncertain observations and reconstructs\nenvironments with remarkable completeness and fidelity. We also demonstrate the\nutility of this uncertainty-aware approach by enhancing SOTA neural SLAM\nsystems through an active ray sampling strategy. Extensive evaluations of\nNARUTO in various environments, using an indoor scene simulator, confirm its\nsuperior performance and state-of-the-art status in active reconstruction, as\nevidenced by its impressive results on benchmark datasets like Replica and\nMP3D.\n\n\n    provide a text summary for the research papers.\n    in format\n    ## Title\n    - content in points\n\n-----------------------------------\n    - (optional) supporting images or diagrams\n\n## NARUTO: Neural Active Reconstruction from Uncertain Target Observations\n- This paper presents a novel neural active reconstruction system called NARUTO, which is designed to reconstruct environments with high fidelity and completeness.\n- The system uses a multi-resolution hash-grid as the mapping backbone, which is chosen for its exceptional convergence speed and capacity to capture high-frequency local features.\n- The centerpiece of the work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment.\n- By harnessing learned uncertainty, the authors propose a novel uncertainty aggregation strategy for goal searching and efficient path planning.\n- The system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity.\n- Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction. (optional) Supporting images or diagrams can be included here.\n\n## References\n- [1] \"NARUTO: Neural Active Reconstruction from Uncertain Target Observations\" by [Author 1], [Author 2], etc.\n- [2] \"Title of Paper 2\" by [Author 3], [Author 4], etc.\n- ...\n\nNote that the references are in a consistent format, and include the titles of the papers, authors' names, and publication information. The references can be formatted using a citation style such as APA, MLA, or Chicago.","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =     257.10 ms\nllama_print_timings:      sample time =     608.92 ms /   303 runs   (    2.01 ms per token,   497.60 tokens per second)\nllama_print_timings: prompt eval time =    6077.33 ms /   272 tokens (   22.34 ms per token,    44.76 tokens per second)\nllama_print_timings:        eval time =   16211.83 ms /   302 runs   (   53.68 ms per token,    18.63 tokens per second)\nllama_print_timings:       total time =   23720.18 ms /   574 tokens\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}